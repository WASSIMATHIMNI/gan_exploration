{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply a gan to match a Gaussian 0,1 (noise) to a Gaussian 10,1 (real data distribution) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function to generate noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated_data_sample(n):\n",
    "    return torch.Tensor(np.random.normal(0, 1, (n,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function to retreive real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real data here follows a Gaussian Distribution of mean 10, std 1\n",
    "def get_real_data_sample(n):\n",
    "    return torch.Tensor(np.random.normal(10, 1, (n,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6251]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_generated_data_sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11.4471]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_real_data_sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generator and Discriminator Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.lt1 = nn.Linear(1, 50)\n",
    "        self.lt2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.lt1(x))\n",
    "        return self.lt2(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lt1 = nn.Linear(1, 50)\n",
    "        self.lt2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lt1(x))\n",
    "        return F.sigmoid(self.lt2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = optim.Adam(D.parameters(), lr=1e-3)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Our Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(num_epochs = 0):\n",
    "    global batch_size\n",
    "    \n",
    "    for d_index in range(num_epochs):\n",
    "\n",
    "        # 1. Train Discriminator on real+fake\n",
    "        D.zero_grad()\n",
    "\n",
    "        #  A: Train D on real\n",
    "        d_real_data = Variable(get_real_data_sample(batch_size))\n",
    "        d_real_decision = D(d_real_data)\n",
    "        d_real_error = loss(d_real_decision, Variable(torch.ones(batch_size,1)))  # ones = true\n",
    "        d_real_error.backward() # compute/store gradients, but don't change params\n",
    "\n",
    "        #  B: Train D on fake\n",
    "        d_gen_input = Variable(get_generated_data_sample(batch_size))\n",
    "        d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        d_fake_decision = D(d_fake_data)\n",
    "        d_fake_error = loss(d_fake_decision, Variable(torch.zeros(batch_size,1)))  # zeros = fake\n",
    "        d_fake_error.backward()\n",
    "\n",
    "        d_optimizer.step()    # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "\n",
    "        return d_real_error+d_fake_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(num_epochs = 0):\n",
    "    global batch_size\n",
    "    \n",
    "    for g_index in range(num_epochs):\n",
    "\n",
    "        # 2. Train Generator on D's response to fake data\n",
    "        G.zero_grad()\n",
    "\n",
    "        gen_input = Variable(get_generated_data_sample(batch_size))\n",
    "        g_fake_data = G(gen_input)\n",
    "        d_fake_decision = D(g_fake_data)\n",
    "        \n",
    "        # here we want to fool, so we need to make the generator go towards producing 1s\n",
    "        \n",
    "        \n",
    "        g_error = loss(d_fake_decision, Variable(torch.ones(batch_size,1))) \n",
    "\n",
    "        g_error.backward()\n",
    "        g_optimizer.step()  # Only optimizes G's parameters\n",
    "\n",
    "        return g_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        d_loss = train_discriminator(2) #train discriminator with a 2:1 ratio\n",
    "        g_loss = train_generator(1)\n",
    "\n",
    "\n",
    "        if epoch % print_interval == 0:\n",
    "            print(\"{}: Total Loss: {} / D: {} / G: {}\".format(epoch,(d_loss+g_loss).item(),d_loss.item(),g_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4000\n",
    "batch_size = 64\n",
    "d_steps = 2  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "g_steps = 1\n",
    "print_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Total Loss: 1.941429615020752 / D: 1.3334829807281494 / G: 0.6079465746879578\n",
      "200: Total Loss: 2.1247782707214355 / D: 1.3338627815246582 / G: 0.7909154295921326\n",
      "400: Total Loss: 2.084935188293457 / D: 1.3824303150177002 / G: 0.7025049924850464\n",
      "600: Total Loss: 2.0343194007873535 / D: 1.432154655456543 / G: 0.6021648049354553\n",
      "800: Total Loss: 2.19767427444458 / D: 1.4093291759490967 / G: 0.7883450984954834\n",
      "1000: Total Loss: 1.9563608169555664 / D: 1.3526643514633179 / G: 0.6036964654922485\n",
      "1200: Total Loss: 2.1178717613220215 / D: 1.3607864379882812 / G: 0.757085382938385\n",
      "1400: Total Loss: 2.151245355606079 / D: 1.4218370914459229 / G: 0.729408323764801\n",
      "1600: Total Loss: 1.9929895401000977 / D: 1.3996505737304688 / G: 0.5933390259742737\n",
      "1800: Total Loss: 2.101590156555176 / D: 1.3573167324066162 / G: 0.7442735433578491\n",
      "2000: Total Loss: 2.146493911743164 / D: 1.4014475345611572 / G: 0.7450463771820068\n",
      "2200: Total Loss: 1.9973479509353638 / D: 1.3951616287231445 / G: 0.6021863222122192\n",
      "2400: Total Loss: 2.114441394805908 / D: 1.3660171031951904 / G: 0.7484241724014282\n",
      "2600: Total Loss: 2.1437714099884033 / D: 1.4119963645935059 / G: 0.7317751049995422\n",
      "2800: Total Loss: 2.016721487045288 / D: 1.3682920932769775 / G: 0.6484293341636658\n",
      "3000: Total Loss: 2.050691604614258 / D: 1.3990390300750732 / G: 0.6516525149345398\n",
      "3200: Total Loss: 2.1574771404266357 / D: 1.3658591508865356 / G: 0.7916179299354553\n",
      "3400: Total Loss: 2.1401946544647217 / D: 1.4165334701538086 / G: 0.7236612439155579\n",
      "3600: Total Loss: 2.0134501457214355 / D: 1.3659758567810059 / G: 0.6474742293357849\n",
      "3800: Total Loss: 2.0221095085144043 / D: 1.4024078845977783 / G: 0.6197016835212708\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.9681])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated = G(torch.Tensor([z]));generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4938])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence = D(G(torch.Tensor([z]))); confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
